片上寄存器是计算机中最快的存储器，它保存着处理器所需要的工作数据。寄存器是以与 CPU 相同的方法制备的，与 CPU 的时钟频率相同，且与 CPU 其他部分之间的数据通常都较短。此外，片上寄存器可以直接被 CPU 访问，而其他存储器需要有一个过程，比如存储器管理、地址翻译、复杂的数据缓冲和控制机制。因此，寄存器速度很快，但是 CPU 只有少量寄存器可以用来储存工作数据和状态信息，而且寄存器不能储存程序。

主存是 Cache 的下一级存储设备，现代计算机大多使用 DRAM 来实现随机访问的主存储器。
因为主存通常不能容纳处理器所需的所有的程序和数据，计算机采用了一种被称作虚拟存储器的存储管理系统，其中主存中仅包括当前使用的数据，那些不使用的数据依然保存在硬盘上。当处理器需要的数据不在主存中时，操作系统开始介入，并使主存和磁盘之间交换一个页的数据，其典型大小为 4KB-64KB。
主存是易失性的存储器，程序和数据需要保存在非易失性存储器中，即一般使用硬盘存储。

存储系统的下一个层次是光存储。由 CD、DVD 或者蓝光光盘构成。光盘存储器将数据以凹痕的方式记录在塑料盘片的螺旋轨迹中，并使用激光根据凹痕处是否有反光来读取数据。光盘的读取速度比硬盘要慢，这主要是因为 CD 或 DVD 盘片的转速与硬盘片旋转的速度相差较大，而其存储容量也并不比硬盘大。

本章包括 Cache 和虚拟存储系统，是因为它们完成相同的任务。两者都使用少量快速存储器，将它们与大容量慢速存储器相结合，使得整个存储系统好似大容量的快速存储器。两种技术都涉及从计算机地址映射到数据在存储器中的实际位置。Cache 和虚拟存储器之间的区别主要体现在速度和控制机制上。Cache 操作需要 ns 量级，由硬件自动管理，而虚拟存储器操作需要 ms 量级，由操作系统管理。

例子：
LDR r1, = time;
LDR r2,[r1];
ADD r1,r1,#1
STR r2,[r1]
这段指令要做的是读出 time 然后将 time 加 1 再写回 time。
计算机给出的操作数地址是一个逻辑地址。将这个地址传给快速的 Cache，在那里试图访问数据。如果数据在 Cache 中，就从 Cache 中获取数据。否则，如果数据在较慢的主存储器中，该数据将从主存交给计算机和 Cache。该过程由硬件实现，并对用户和操作系统来说是不可见的。有时，数据也不在主存储器中，而在硬盘上，此时，虚拟存储器机制将其从磁盘拷贝到主存储器。它当然也被拷贝到了 Cache。从磁盘传输到主存储的数据可以存放到主存储器的任意位置。这就需要存储器管理单元 MMU 将来自计算机的逻辑地址（即计算机认为数据所在的地方）转换为物理地址（即操作系统把该数据存放在存储器中的地址）。虚拟存储器管理是操作系统完成的主要任务之一，需要存储器管理单元的专用硬件与操作系统密切合作。

为什么要有 Cache 存储器
慢速存储器如何影响 CPU 性能？
如果存储器不能在当前周期向 CPU 提供所需访问的数据或指令，存储器必须返回一个信号通知 CPU 等待。CPU 通过在机器周期中插入空闲或等待指令来阻止其执行下一个操作。在等待状态下，CPU 将停止正常操作，因此慢速存储器将严重降低其性能。
Cache 的意义？
Cache 高速缓存本来没什么内在价值。总线用来分布数据，硬盘用来存储大量的数据。Cache 的作用仅仅是隐藏了存储器延迟。如果存储器足够快，就可以不需要 Cache。
Cache 能被使用的前提？
各个给定的存储位置被访问的概率是不同的，因为有些地方更容易被访问。由于程序和其数据结构的性质，处理器所需的数据往往在存储器中聚集在一起。例如，堆栈经常被访问，某些函数与其他函数相比经常被调用。这种现象被称为访问的局部性，这使得 Cache 的使用成为可能。
一些地址被称为出现了空间局部性，这是因为它们聚集在相同的存储器区域内。程序员和编译器会针对空间局部性进行特殊考虑。假设某程序包括变量 P、Q、R，其中 P 是整数，R 是由 8 个整数组成的数组，Q 是另一个整数。另外，假设 P 和 R 经常被访问而 Q 很少被访问。如果数据的声明顺序为 P、R、Q，则这两个经常访问的变量在存储器中是相邻的，可以被缓存在一起。
一些地址被称为出现了时间局部性，这是因为它们在短时间内被多次访问，例如循环。

Cache 的基本工作原理
Cache 存储器与处理器的地址总线和数据总线相连，并与更大容量的主存储器并行工作。Cache 中的数据在主存储器（即 DRAM）中也存在副本。
Cache 使用 Cache 控制器来确定 CPU 要访问的操作数是否在 Cache 中，是否需要从主存储器中调取。当给 Cache 控制器一个地址后，Cache 控制器返回一个信号（hit 或 miss），确定 Cache 是否命中。

Cache 存储器的性能
Cache 系统最重要的参数就是命中率 h，它定义为所有访问操作在 Cache 中命中的概率，由系统操作统计结果决定。访问局部性使得命中率通常较高，基本保持在 98%左右。
不使用 Cache 和使用 Cache 所用时间之比记为加速比，加速比对命中率敏感。只有当 h 接近 90%时，Cache 的性能影响才十分显著。该结论与常识一致。

Cache 的复杂性
1：实际上，当数据不在 Cache 上时，存储系统通常会装载整个 Cache 块而不仅仅是其中一个数据项。
2：必须考虑 Cache 读写操作之间的差异。
3：大多数高性能系统在一级 Cache 这个层次具有单独的数据 Cache（D-Cache）和指令 Cache（I-Cache），通过允许指令和数据同时传输来提高处理器带宽。
4：我们通常假设主存储器系统具有统一的访问时间，然而实际上，由 DRAM 组成的现代高性能存储系统的访问时间相当复杂，因为信息往往是以突发的方式访问，此时第一个存储器访问时间往往要比后续访问时间长，

Cache 的组织
Cache 设计的根本问题是如何构建一个存储器来存放来自大容量存储器任意位置的一组数据元素。
全相联映射 cache
在设计任何一个存储系统时需要问的第一个问题是：基本数据大小是多少？主存储器处理单位数据的大小与机器的基本字长相同。
那么 Cache 基本数据的大小是多少呢？虽然计算机可以从 Cache 中读取一个字，但该字的大小并不是 Cache 基本存储单位的大小。Cache 基本存储单元为包含几个连续字的块。假定 Cache 是按照字的粒度来组织的。当访问某条指令时，如果它不在 Cache 中，则必须从主存储器中读取，然而，下一条指令又有可能失效。因此需要比一个字大的 Cache 基本存储单元。
一块数据由多个字构成。一个块中的某个字的位置可以由其块地址以及块内偏移给出。
主存储器分为 n 块，每块大小为 X 字节。同样，Cache 也被分为 m 块，每块 X 字节。人们希望 Cache 中的数据可以来自主存储器的任何地方，即 Cache 允许主存储器的任意一块数据存放在其任意一块中。人们使用标记来完成这件事，即 Cache 中的第 i 块需要一个标志来唯一标识其与主存储器的某一块相关联。例如，主存储器有 2 的 n 次方块，就需要 n 位的标志。这种 Cache 就被称为全相联映射 Cache。
这种 Cache 中的数据没有排序，数据可以存放在任意位置，即没有地址的概念。每个数据有两个项，关键字和数据元素。每个数据根据关键字存储和检索。
但是，这种方法有一个问题，例如，现在有来自主存储器的 1M 个块的地址，要如何映射到仅有 4096 个块的 Cache 上呢？用上面的方法的话我们要自建查找表，表要有 1M 个项，每个项要有 12 位用来映射 Cache 指针，还要有 1 位标志是否在 Cache 上。这样 CPU 在查找时，拿着主存储器的地址看表，再进行下一步。这样一来浪费存储空间，二来查表速度有限制。

全相联映射的优势：Cache 中的数据没有排序，数据可以存放在任意位置，即没有地址的概念。
全相联映射的问题：空间，时间。

相联存储器
相联 Cache 名称来源于其使用相联存储器来存储标志（取代表）。但是相联存储器 n 位输入并不需要对应 2 的 n 次方个唯一内部位置。n 位输入交给相联存储器并与 Cache 每个位置上的标志域进行比较。中则中，不中，则根据算法进行替换。（比如最近最少使用）。
相联存储器很贵，因为它需要大量的并行逻辑将输入关键字与当前存储的关键字同时进行匹配。
相联 Cache 的两种失效：一种是第一次访问的强制失效（因为是空，除非预先调入数据）；第二种是容量失效，满了。

直接映射 Cache
组织 Cache 最简单的方法就是采用直接映射，它依赖于一种简单的算法将主存储器中的数据块直接映射到 Cache 中的块。
1：将主存块被分为若干个组，每组划分成若干块。其中，组的大小与 Cache 的大小一致，Cache 也划分成同样的若干块。换句话说，主存被划分成了若干个 Cache。
2：来自 CPU 的地址自然被划分为组地址、块地址和字地址。
3：除了 Cache 存储器本身，这种方法还要用到 Cache 标志 RAM，Cache 标志 RAM 和 Cache 具有相同的块数，每块数据仅仅存储块地址和其对应的组地址。
4：Cache 标志 RAM 是一种由高速随机访问存储器和数据比较器构成的特殊设备。
流程：CPU 访问的块地址传入 Cache 标志 RAM，获得对应的组地址，送往数据比较器并与地址总线上的组地址进行比较。如果 CPU 给出的组地址和被访问块的标志相匹配，Cache 标志 RAM 返回命中信号。若访问组 Y 的块 X 失效，主存储器组 Y 的块 X 将被加载到 Cache 中的第 X 块，当新块载入时，直接将失效位置对应的块淘汰出 Cache。
优点：并行性。由于保存数据的 Cache 存储器和 Cache 标志 RAM 是独立的，它们可以被同时访问。
缺点：对被缓存数据的位置敏感。比如一共有 10 个块，0-9，但是每次都访问 2 号，这样的话即使其他号是空位，根据算法也会频繁替换 2 号位。即使 Cache 没有存满，在访问主存储器不同组的相同块号的两个块时，会导致块在 Cache 和主存储器间进行交换。该问题会导致较低的 Cache 利用率和高失效率。这种在 Cache 没有存满的时候也发生替换的情况称为冲突失效。
特点：即使在一个简单的系统里，直接映射 Cache 中的元素也可能很容易地被替换。比如主存储器 2000 行代码，Cache1000 容量，然后我只在第 0 行和第 1000 行写上两句话，Cache 居然要替换。

伪相联 Cache
伪相联 Cache 在逻辑上把直接映射 Cache 分为上下两个区。
当直接映射 Cache 发生冲突失效时，对失效地址的高 n 位取反（即从另外的区找）。
其问题在于，有时需要访问两次 Cache，会有一快一慢两种命中时间。

组相联 Cache
直接映射 Cache 只为每个块 i 分配了一个位置。如果使用两个直接映射 Cache 并行工作，就可以使块 i 进入它们中的任意一个。如果使用 n 个直接映射 Cache 并行工作，就可以使块 i 进入 n 个位置中的任意一个。这就是 n 路组相联 Cache。
Cache 容量比较小时，相联度是一个重要因子。当 Cache 容量达到 256KB 时，相联度就不十分显著了。

Cache 失效
Cache 失效有三种：强制失效，容量失效，冲突失效。
强制失效
第一次访问数据块时，它不一定在 Cache 中，这是无法避免的。
有些处理器通过预测来避免强制失效，在需要的数据被访问之前就将其纳入 Cache。
容量失效
工作集（当前程序所使用的块）比 Cache 大，所需的数据无法全部驻留在 Cache 中。如果程序足够大，当 cache 被充满，下一次的访问就会导致容量失效。
冲突失效
这是最浪费的一种失效，这是因为 Cache 还未存满时就会发生。
一般都是 Cache 组织形式的副作用导致新的数据与 Cache 中的数据发生冲突。m 路组相联 Cache 的 m 个组的第 i 块都被占用，此时再来第 i 块，就会发生冲突失效。

Cache 污染
有时，访问导致某一块被替换，新的一块被载入。但这个新载入的块可能再也不会被访问，却占用了可以保存其他经常需要被访问块的空间，这种情况被称作 Cache 污染。

其他 Cache 技术
Victim Cache
Victim 有遇难者的意思，Victim Cache 是一个小小的 Cache，用来保存最近被替换出 Cache 的项目。
它与原 Cache 并行访问，理想情况下，它是全相联的。因为它的容量非常小，而且可被替换的块有限，所以可以使用全相联方式构建。
它可以减少冲突失效，缓解容量失效。
在循环程序中效果显著。
Selective Victim Cache
在 Victim Cache 基础上，加入了预测算法。
在此方案中，进入第一级缓存的传入块通过使用基于过去使用历史的预测方案，有选择地放置在主缓存或受害者缓存中，此外，主缓存和受害者缓存之间的块交换也是有选择地执行的。
Annex Cache
与 Victim Cache 很类似，Victim Cache 给予数据块第二次机会，Annex Cache 要求想进入 Cache 的数据证明自己的价值；Victim Cache 位于主 Cache 之后，Annex Cache 位于主 Cache 之前。
Annex Cache 会保存导致冲突失效的数据块，直到它们多次被访问证明其价值比主 Cache 中的那份还要高的时候，才被允许进入主 Cache。
流缓冲
利用访问的局部性原理，在失效发生时，流缓冲预取访问块后的若干块，如果处理器稍后访问这些数据，就可以从流缓冲中获取。流缓冲在访问数据之前预取数据，所以可以减少强制失效。
其开销在于预取了可能并不需要的数据，从而增加了存储器总线负载。
条缓冲
在当前数据基础上适当偏移预取下一个数据。在读取诸如数组这种数据结构时很有效果。

Cache 设计的要点
多级 Cache
16KB 的一级 Cache 和 1KB 的二级 Cache 与 1KB 的一级 Cache 和 16KB 的二级 Cache 性能几乎相同。
物理 Cache 和逻辑 Cache
在具有存储器管理单元（memory management unit，MMU）的计算机中，操作数的逻辑地址由 CPU 生成并被 MMU 转换成主存储器使用的物理地址。

这就产生一个问题，Cache 是应该放在 CPU 和 MMU 之间，还是应该放在 MMU 和主存储器之间。
如果在 CPU 数据终端上的数据被缓存，则该数据被称为逻辑数据，相应的 Cache 被称为逻辑 Cache。
如果数据经过 MMU 地址转换后被缓存，则该数据被称为物理数据，相应的 Cache 被称为物理 Cache。

物理 Cache 比逻辑 Cache 有更长的访问时间，这是因为物理 Cache 直到 MMU 进行了地址转换后才可以进行数据访问，而逻辑 Cache 可以直接被访问。
逻辑 Cache 有时需要刷新，比如在多任务系统中发生了上下文切换且要执行一个新任务，此时逻辑地址和物理地址之间的映射关系被改变，逻辑 Cache 必须刷新，而物理 Cache 则不需要刷新。          
指令 Cache 和数据 Cache
1. 将指令 Cache 和数据 Cache 分开是很有意义的，因为它们有不同的特性，指令 Cache 不会被修改。
2. 将指令 Cache 和数据 Cache 分开实现，可以提高 CPU 与存储器间的带宽，因为指令和数据可以同时读取。
混合 Cache 也并非一无是处，以下是上述三者的特点：
指令 Cache 可以为产生指令流而优化；数据 Cache 可以为读写操作优化。
指令 Cache 不支持自修改代码；混合 Cache 支持自修改代码。（*自修改代码：早期计算机代码可以在运行时动态变化。这种代码难以阅读和管理，而且不适用于 Cache 或存储器管理的环境工作*）
混合 Cache 需要更快速的存储器；混合 Cache 更加灵活。（分离 Cache 可能出现指令 Cache 满而数据 Cache 还空着一半的情况）
分离 Cache 可以通过同时操作提高 CPU 与存储器间的带宽。
今天的大多数处理器都有分离 Cache，即使在多级 Cache 系统中。一般，高级别 Cache（三等）可以采用混合 Cache，而低级 Cache（一二）一般都是分离 Cache。
e.g. AMD Barcelona 体系结构中，三级 Cache 被多个核共享，加载数据时将直接从三级 Cache 传给一级 Cache 而不通过二级 Cache。被传送的数据要么由于需要被多个处理器使用而保持在三级 Cache 中，要么由于不需要共享而被删除。与二级 Cache 类似，三级 Cache 不保存来自存储器的数据，而是保存被替换出二级 Cache 的数据。
Cache 的电气特征
半导体随机访问存储器分为两大类：静态和动态。
静态存储器利用传统数字逻辑将一位数据存储在一个触发器中。其特点是低功耗、高速和只要有电就能保留数据。但是它需要 6 个晶体管来存储一个比特位，意味着其物理尺寸要比 DRAM 大得多、意味着其造价昂贵且容量小。一般 Cache 由静态 RAM 构造。
动态存储器通过对单个晶体管的电容充上电荷来保存数据。这使得 DRAM 非常便宜且紧凑，可以用来构造大容量存储器。但是它需要更多的电量进行控制，且电容中的电量在几 ms 内就会漏光。为了保存 DRAM 中的数据，必须每隔 4ms 左右定期读取存储单元中的数据并将其写回。
Cache 一致性
如果 Cache 中的数据被修改，而主存储器的数据没有被修改（反之亦然），可能会导致严重的后果。
比如 I/O 控制器将主存储器上的一块数据移动到磁盘，但是此前处理器刚刚更新了 Cache 中的数据而尚未写回到主存储器，此时磁盘上的内容将是过时的。
比如多处理器系统中，两个处理器各自的 Cache 缓存了同一份数据，A 更新并写回了，但是 B 不知道的话，就会出现问题。
Cache 一致性意味着不同 Cache 和主存储器中的数据必须保持同步。
有些处理器通过一种被称为总线侦听的技术来保持 Cache 的一致性。处理器通过监视地址流和数据流发现向主存储器的写访问且自身的 Cache 中也有一个副本的情况。一旦发现，就将自身 Cache 内容标记为无效或更新 Cache。
块大小
块是 Cache 中的基本存储单位。一个重要的问题是：要多大的块才能获得最佳性能？
最佳的块容量取决于这几个方面：
1. 正在执行的程序的性质。
2. 负责控制处理器和存储器之间的数据流量的总线协议：典型的总线，其每次存储器访问都需要一个地址和一个数据元素。而突发模式的总线，可以发送一个地址，而获得一批连续的数据值。
3. 指令和数据的混合情况：针对指令的最佳块容量不一定与针对数据的最佳块容量相同。
增加还是减少块容量？
增加块容量会提高 Cache 的效率，因为数据对象是由一组连续的字节组成的，可以利用空间局部性原理。然而，当块容量不断增加，命中率也会下降，因为减少了块的数量使得给定对象被缓存的概率降低。
此外，当发生失效调入一个块，它可能包含不经常访问的数据，也可能把经常访问的数据替换出去了。
*有些处理器有可变长度指令，此时如果 Cache 很小，可能会出现一条很长的指令横跨两个 Cache（块交叉），读这样的一条指令竟然需要两次 Cache 访问，这不好*

下面是实验数据：
随着块容量增加，数据 Cache 的失效率首先逐渐变好然后逐渐恶化，直到与 Cache 容量本身相同。
随着块容量增加，指令 Cache 的失效率一直减少。
这说明，访问的局部性对指令的作用比对数据的作用更强。且一般情况下，只要给定 Cache 容量就有一个最佳块容量。
降低失效率策略
1. 按需获取：在失效后调入所需的块。天经地义。
2. 预取：预测未来的需求提前调入。
3. 选择性获取：在主存储器的部分内容不能被缓存时使用（比如多处理器共享的数据）。
Cache 与程序员
1. 访问数组时，以行的顺序访问，而非列。
2. 循环遍历时，提前将下一个或几个 fetch 到 Cache（手动预取）。
写 Cache
写直达策略：在写入 Cache 的同时也改写主存储器中的数据，这称为写直达策略。该策略导致系统变慢，因为写主存储器的时间要比写 Cache 的时间长。不过如果下一个操作是读 Cache，主存储器可以同时完成操作，即写直达策略也不一定那么不堪。
写缓冲策略：保存等待写入存储器的数据，典型的写缓冲保存 4 个地址 / 数据对。一个问题是，这种策略必须保证处理器访问的数据是最新的。一种解决方法是在执行访问前允许写缓冲完成存储器更新。
写回策略：向主存储器的写操作只有在 Cache 块被替换时才会发生，即对 Cache 的写操作并不是每次都导致对主存储器的更新。只有某块由于读失效被替换出去时才将该块写回存储器。此策略要求 Cache 中的每一块都有一个标志来描述当前块的状态。例如一个脏位来表示它调入 Cache 后是否被修改过，如果没有，替换时则不用写回。
*不按写分配策略：写失效时数据不调入 Cache。*
改善 Cache 性能
1. 尽早启动：只要所需块中的字已从存储器中取出，就可以使用，而不必等到整个块都调入 Cache。
2. 关键字优先：字失效后，先从存储器中取出导致失效的那个字，这样存储器可以继续执行，然后再把对应块中其他字读出来成为一个块调入 Cache。
3. 非阻塞 Cache：通常情况下，发生失效时，需要向 Cache 调入新块，此时处理器处于暂停状态。然而，有可能下一次存储器访问并非访问被替换的块。这种情况下，处理器可以继续执行。
虚拟存储器和存储器管理
从许多方面看，存储器是一种扩展的 Cache 技术。
将操作数的逻辑地址转换成主存储器使用的物理地址的过程称为存储器管理。
系统的主存储器构成物理地址空间。计算机逻辑地址空间的大小由指定地址所需的位数决定。
将逻辑地址转换成物理地址需要一个查找表。因为将每个逻辑地址都转换为物理地址确实需要非常大的表，所以存储空间被分成页，每个逻辑页中的地址转换为物理页中的地址。
存储管理系统的基本目的：
1. 实现对物理地址空间大小超过逻辑地址空间大小这样的系统的控制。
2. 实现对逻辑地址空间大小超过物理地址空间大小这样的系统的控制。
3. 存储器保护，包括防止一个用户访问分配给另一个用户的存储空间的机制。
4. 存储器共享，一个程序可以与另一个程序共享资源。
5. 最有效地使用物理地址空间。
6. 将程序员从考虑程序和数据应该位于存储器的何处中解放出来。即程序员可以根据其意愿使用任意的地址，而存储器管理系统会将逻辑地址映射到可用的物理地址。
虚拟存储器的 4 个作用：
1. 支持比物理空间更大的逻辑存储空间。
2. 实现逻辑空间到物理空间的映射。
3. 为逻辑空间中运行的任务分配物理存储器。
4. 更加方便地建立多任务的系统。
多任务系统：多任务系统通过周期性地在任务之间进行切换以支持两个或多个进程同时执行。显然，多任务处理只有当几个任务同时驻留在主存储器中时才是可行的。否则，切换时间过长。
操作系统搜索可用的物理存储器空间，寻找空闲的存储块并将其分配给任务。一个好的操作系统应该有效地执行存储器分配，不允许物理存储器中存在大量未使用的块。存储器分块取决于存储器系统映射实现的方式和操作系统。
存储器映射的一个强大功能是：每个逻辑存储器块可被赋予各种权限。例如：只读、只写、只能通过操作系统或给定的任务访问或在一组任务之间共享。通过确保物理存储器块只能被预先定义的任务访问，可以确保一个任务的执行不会导致另一个任务崩溃。
两种实现存储器管理的基本方法：使用固定大小的存储器块，称为页；另一种使用可变大小的存储器块，称为段。
页：一般都是逻辑地址远远大于物理地址。
将 A 位物理地址分为 N 位的页地址和 X 位的子地址。
将 B 位逻辑地址分为 M 位的页地址和 X 位的子地址。
页表共有 2 的 M 次方项。每一项包含：页地址、驻留位 R、修改位 M、属性和页帧地址。
R 表示，此页是否在物理存储器中。
页故障：当产生逻辑地址且与当前逻辑页相关的 R 被清零时，将发生页故障。典型的微处理器具有总线错误输入引脚，表明存储器访问无法完成。当发生这种情况时，操作系统开始干预。操作系统从磁盘检索包含所需存储器位置的页，将其加载到物理存储器中，并相应地更新页表，被暂停的指令可以继续执行。更新页表需要覆盖一个随机访问物理存储器的页。LRU least recently used。
M 表示，此页是否被写过。被写过的页，如果需要被替换，则要先写回。
抖动：指一种频繁访问资源导致计算机性能急剧下降的行为。很大程度上，是指由于需要加载和重新加载页而导致虚拟存储器系统发生故障的行为。
上述单页表在实际中不存在。因为不可能在 RAM 中构建那么大的页表。所以实际上都是使用两级表。
