在过去的⼆⼗年中，图形硬件发⽣了不可思议的变化。第⼀个包含硬件顶点处理的消费级芯⽚ NVIDIA GeForce256 于 1999 年发布。NVIDIA 创造了图形处理单元（graphics processing unit，GPU）这个术语，⽤来区别 GeForce256 和过去所使⽤的光栅化芯⽚，并将这个术语沿⽤了下来。在接下来的⼏年时间⾥，GPU 从⼀个可配置的复杂固定功能管线，发展为⼀个⾼度可编程的“⽩板”，开发者可以在这个⽩板上实现⾃⼰的图形算法。各种可编程的着⾊器是控制 GPU 的主要⼿段。为了获得更⾼的效率，渲染管线中的有些部分仍然只是可配置的，⽽并⾮是可编程的，但是 GPU 的整体发展趋势是可编程性和灵活性。

GPU 专注于⼀组⾼度并⾏化的任务，从⽽获得了很⾼的处理速度，它使⽤专⻔的硬件来实现 z-buffer，来能够快速访问纹理图像和其他缓冲区，还可以快速寻找哪些像素被⼀个三⻆形所覆盖。我们将在第 23 章，讨论这些硬件单元是如何实现各⾃的功能的。

着⾊器核⼼是⼀个⽤于执⾏某些相对独⽴任务的⼩型处理器，例如将⼀个世界空间中的顶点位置，变换到屏幕空间中；或者是计算⼀个像素（被三⻆形覆盖）的颜⾊。

#### 延迟问题
⾸先我们需要知道⼀点，由于在存储中访问数据需要花费⼀定时间，因此延迟是所有处理器都会⾯临的⼀个问题。考虑延迟的⼀个基本⽅法是，数据距离处理器越远（物理意义上的距离），访问所需要等待的时间就越⻓。相对于本地寄存器⽽⾔，访问内存碎⽚中的信息则需要花费更多的时间。这个问题的关键在于，等待数据检索意味着此时处理器处于停滞状态，这会降低性能表现。

不同的处理器架构使⽤了不同的策略来避免停滞。CPU 经过优化，可以处理⼤量的数据结构和⼤型代码段，CPU ⼀般都具有多个处理器，每个处理器都以串⾏的⽅式来执⾏代码，但是有限的 SIMD 向量处理是⼀个⼩例外。
为了最⼩化延迟所带来的影响，CPU 芯⽚中的⼤部分⾯积都是⾼速的本地缓存，这些缓存中存满了接下来可能会⽤到的数据。CPU 还会使⽤⼀些智能技术来避免停滞，例如分⽀预测、指令重排序、寄存器重命名和缓存预取等。

⽽ GPU 则采⽤了不同的策略，GPU 芯⽚中的很⼤⼀⽚⾯积都是⼤量的处理器，也叫做着⾊器核⼼，GPU 芯⽚中通常会有数千个着⾊器核⼼。
GPU 是⼀个流处理器，它会依次处理有序的相似数据。由于数据的相似性（例如⼀组顶点或者像素），因此 GPU 可以通过⼤规模并⾏的⽅式来处理这些数据。另⼀个重要因素是，这些着⾊器调⽤都是尽可能独⽴的，即它们不需要来⾃邻近调⽤的信息，也不需要共享可写⼊的内存位置。有时候为了使⽤⼀些新功能，这个规定会被打破，但是这种例外会带来潜在的额外延迟。

GPU 专⻔对吞吐量（throughput）进⾏了优化，吞吐量指的是数据能够被处理的最⼤速度。但是这种快速处理是有代价的，由于⽤于缓存和控制逻辑的芯⽚⾯积较少，因此每个着⾊器核⼼的延迟，通常都会⽐ CPU 处理器所遇到的延迟要⼤。

##### 避免延迟的方法
- 当遇到会令着⾊处理器停滞的指令时，我们可以通过切换并执⾏其他⽚元程序的⽅式，来让 GPU 时刻保持忙碌，从⽽避免延迟。
- 更进⼀步，GPU 可以将指令执⾏的逻辑与数据分离开来，这种设计叫做单指令、多数据（SIMD），这种设计会在固定数量的着⾊器程序上，以⼀个固定的步⻓来执⾏完全相同的指令。与使⽤⼀个独⽴的逻辑单元和调度单元来执⾏每个程序相⽐，SIMD 的优势在于，可以使⽤更少硅芯⽚（也意味着更⼩的功耗）来⽤于处理数据和进⾏切换。使⽤现代 GPU 的术语来说，每个⽚元的像素着⾊器调⽤都可以被称为⼀个线程，但是这⾥所说的线程不同于 CPU 上的线程，它还包括⽤于存储着⾊器输⼊数据的存储空间，以及⽤于着⾊器执⾏的任何寄存器空间。这些使⽤相同着⾊器程序的线程会被打包成组，NVIDIA 将其称作为⼀个 warp，AMD 将其称作为⼀个 wavefront。⼀个 warp/wavefronts 负责调度⼀定数量的 GPU 处理核⼼，可能是 8 到 64 个，并且都会使⽤ SIMD 处理。每个线程都会被映射到⼀个 SIMD 通道。
e.g. ⼀个三⻆形的⽚元（被称作线程），被打包成很多个 warp。为了简化表示，上图中的每个 warp 都只包含四个线程，⽽真正的 warp ⼀般包含 32 个线程。这个等待执⾏的着⾊器程序中包含五个指令。这四个处理器为⼀组，开始执⾏第⼀个 warp 中的指令，直到在遇到“txr”指令时，检测到了⼀个停滞状态，这个指令需要花费⼀些时间来从存储中获取数据。然后第⼆个 warp 会被切换到处理器中，同样执⾏着⾊器程序中的前三条指令，直到再次检测到⼀个停滞状态，第三个 warp 也同样如此。然后处理器会再次切换回第⼀个 warp，继续进⾏执⾏，如果这个时候“txr”指令所需要的数据还没有返回的话，那么处理器才会真正的停滞下来，直到这些数据可⽤的时候。每个 warp 都会按照顺序轮流执⾏。

在我们的这个简单例⼦中，在存储中读取纹理所带来的延迟，可能会导致 warp 进⾏交换。由于这个切换的成本⾮常低，所以实际上的 warp 可以通过交换来获得更低的延迟。

warp 交换是 GPU 上最重要的延迟隐藏技术，这个过程的⼯作效率还涉及到好⼏个其他因素：
- 线程的数量过少，导致warp的创建过少，这可能无法有效隐藏延迟
- 着色器程序的结构也会影响，比如如果每个线程使用了更多的寄存器数量，那就意味着GPU能够同时存在的线程数量和warp数量也就越少。
- 存储读取的频率也会产生影响。
- if语句和循环语句产生的动态分支。如果一个warp里所有线程都走了同一个分支，那确实没有影响，但是其中只要有一个线程走了另一个分支，那么warp就必须走完两个分支，然后根据线程的需求丢弃一个分支的结果。这种情况叫做线程发散，它意味着有一些线程要去执行一个循环或者走另类的分支，导致其他的线程空转。

所有的 GPU 实现都应⽤了这些架构思想，虽然这样会导致系统具有严重的限制，但是这也提供了⼤量的每瓦计算能⼒。

#### 渲染管线可编程性视角
顶点着⾊器是⼀个完全可编程的阶段，它⽤于实现渲染管线中的⼏何处理阶段。曲⾯细分阶段和⼏何着⾊器都是可选的阶段，也是完全可编程的阶段，但并不是所有的 GPU 都⽀持这两个阶段，尤其是移动设备上的GPU。裁剪，由固定功能的硬件实现。屏幕映射受到窗口和视口设置的影响，其内部包含了一个简单的缩放和重定位功能。三角形设置和三角形遍历过程都是由固定功能的硬件实现。像素着色器是一个完全可编程的阶段。合并阶段不是可编程的但是是高度可配置的。

#### 可编程着色器
现代的着⾊器程序都使⽤了统⼀的着⾊器设计，这意味着顶点着⾊器、像素着⾊器、⼏何着⾊器以及与曲⾯细分相关的着⾊器，都共享⼀个通⽤的编程模型，它们内部的指令集架构（instruction set architecture，ISA）都是相同的。
这种架构背后的思想是，这些着⾊处理器可以⽤于执⾏不同的任务，⽽ GPU 可以视情况来对这些处理器进⾏分配。比如⼀个具有独⽴顶点着⾊器核⼼池和像素着⾊器核⼼池的 GPU 意味着，为了保持所有的核⼼都处于忙碌状态，那么需要预先规定顶点着⾊器和像素着⾊器的任务⽐例，而通过统⼀的着⾊器核⼼，GPU 可以视情况来平衡这个负载。

着⾊器使⽤了类 C 的着⾊器语⾔进⾏编写，例如 DirectX 中的 High-Level Shading Language（HLSL）和 OpenGL 中的 OpenGL Shading Language（GLSL）。为了提供硬件独⽴性，HLSL 可以被编译成虚拟机器码，它也被叫做中间语⾔（intermediate language，简称为 IL 或者 DXIL），这种中间表示还可以允许着⾊程序进⾏离线编译和存储，它会被 GPU 驱动程序转换为特定的 ISA。游戏主机上通常会避免这个中间语⾔步骤，因为主机系统只有⼀个 ISA。

在现代 GPU 上，同样原⽣⽀持 32 位整数和 64 位浮点数。浮点向量通常⽤来表示位置（ xyzw ）、向量、矩阵中的某⼀⾏、颜⾊（ rgba ）或者纹理坐标（ uvwq ）等。整数通常⽤来表示计数器、索引或者位掩码等。诸如结构体、数组和矩阵等聚合类型，也同样被 GPU ⽀持。

⼀次 draw call 会调⽤图形 API 来绘制⼀组图元，渲染管线也会相应执⾏它所对应的着⾊器。
每个可编程的着⾊器阶段都包含两种类型的输⼊：
统⼀输⼊，它是指在⼀次 draw call 中不会发⽣改变的常量；
可变输⼊，来⾃三⻆形的顶点或者光栅化的数据。
例如：像素着⾊器中的光源颜⾊可能是⼀个统⼀的值，在⼀次 draw call 中并不会发⽣变化；⽽三⻆形的表⾯位置则会根据屏幕上的每个像素发⽣变化，因此这个位置数据是可变的。
纹理是⼀种特殊的统⼀输⼊，在过去，纹理总是会被视为⼀个应⽤于⽹格表⾯的颜⾊图像，⽽如今纹理可以是任意的阵列数据。

底层虚拟机为不同类型的输⼊提供了不同的寄存器，常量寄存器⽤于存储统⼀输⼊，其数量远⼤于可变寄存器。这是因为可变输⼊输出只需要为每个顶点或者每个像素存储独⽴的数据即可，因此其所需要的数量是天然有限的。⽽统⼀输⼊在⼀次 draw call 中只会存储⼀次，然后会在所有的顶点或者像素中进⾏重⽤。虚拟机还有⽤于临时存储的通⽤临时寄存器，所有类型的寄存器，都可以使⽤存储在临时寄存器上的整数数值，来进⾏数组索引。

图形计算中的常⻅操作和运算都可以在现代 GPU 上⾼效执⾏，着⾊语⾔通过操作符来暴露最常⽤的操作，例如加法和乘法的操作符是 + 和 ∗ ；其余的操作可以通过使⽤内置函数来提供，这些内置函数针对 GPU 进⾏了专⻔优化，例如： atan() ， sqrt() ， log() 。还有⼀些函数可以提供更加复杂的操作，例如向量的标准化、求反射向量、向量的叉乘、求矩阵的转置以及⾏列式等。

着⾊器⽀持两种类型的流程控制，包含静态流程控制以及动态流程控制。其中静态流程控制的分⽀情况会基于统⼀输⼊的值，这意味着在⼀次 draw call 中，该代码的流程是恒定不变的。静态流程控制最主要的好处在于，它可以在各种不同的情况下（例如不同数量的光源）使⽤相同的着⾊器，并且在这个过程中没有任何的线程发散，因为所有调⽤都会进⼊相同的代码路径。动态流程控制则基于可变输⼊的值，它意味着每个⽚元都可以执⾏不同的代码，其功能⽐静态流程控制更加强⼤，但是也更消耗性能，尤其是当着⾊器调⽤之间，代码流程不规则变化时。

#### 可编程着⾊及其 API 的演变
可编程着⾊框架的想法可以追溯到 1984 年 Cook 所提出的着⾊树。

2001 年初，NVIDIA 推出了 Geforce3 显卡，这是第⼀个⽀持可编程顶点着⾊器的GPU，它通过 DirectX 8.0 来暴露相关接⼝，并可以扩展到 OpenGL。

依赖纹理读取（是指需要使⽤第⼀个纹理读取的返回值，来确定第⼆个纹理读取的位置）和浮点数据对于实现真正的可编程性⾄关重要。

这时的着⾊器不允许包含流程控制（分⽀），如果着⾊器中包含分⽀，那么就需要将两个分⽀都执⾏⼀次，然后在结果中进⾏选择或者插值来模拟分⽀。

Shader Model 2.0 ⽀持了任意的依赖纹理读取，并⽀持 16 位的浮点数值，还扩展了着⾊器资源的范围（例如指令、纹理以及寄存器），着⾊器因此可以⽣成更加复杂的特效，同时增加了对流程控制的⽀持。

随着着⾊器程序⻓度和复杂度的不断增加，这使得使⽤汇编模型来开发着⾊器变得越来越繁琐。

Shader Model 3.0 于 2004 年推出，并增加了动态流程控制，这使得着⾊器更加强⼤。它还将可选的功能特性纳⼊了需求列表，进⼀步扩⼤了可使⽤资源的范围，在顶点着⾊器中添加了对纹理读取的有限⽀持。

着⾊器可编程性的下⼀次跨域也出现在 2006 年底，DirectX 10.0 推出了 Shader Model 4.0，它引⼊了⼏个重要特性，例如⼏何着⾊器和流式输出。Shader Model 4.0 包含了⼀个针对所有着⾊器的统⼀编程模型，即我们在前⽂中描述过的标准着⾊器设计。并且它进⼀步扩⼤了资源范围，同时⽀持了整数类型的数据（包括位运算等操作）。

在 2009 年发布的 DirectX 11 和 Shader Model 5.0 中，增加了曲⾯细分着⾊器和计算着⾊器，计算着⾊器也被叫做 DirectCompute。这次发布还关注了如何提⾼ CPU 多线程处理的效率。OpenGL 在 4.0 版本中添加了曲⾯细分，在 4.3 版本中添加了计算着⾊器。DirectX 和 OpenGL 的发展⽅式不太⼀样，但是⼆者都针对特定的发布版本，设定了⼀定的硬件⽀持级别。

图形 API 的下⼀个重⼤变化是由 AMD 于 2013 年提出的 Mantle API，它是 AMD 和电⼦游戏开发商 DICE ⼀起合作开发的，其核⼼想法是去除⽤于图形驱动程序的⼤量开销，将控制权直接交给开发者。除此之外，该技术还进⼀步⽀持了 CPU 多线程的⾼效处理，这⼀类 API 专注于如何减少 CPU 花费在驱动上的时间，以及如何更加⾼效的利⽤ CPU 的多个核⼼（第 18 章）。

DirectX 12 并没有增加更多的 GPU 功能——DirectX 11.3 和 DirectX12 具有完全相同的硬件特性。这两个 API 都可以⽤于向虚拟现实系统发送并显示图形。DirectX 12 是对 API 的彻底重构，它可以更好的映射到现代 GPU 架构。低开销的驱动程序对于 CPU 驱动所导致性能瓶颈的应⽤程序⾮常有⽤，或者是可以利⽤更多的 CPU 处理核⼼来获得更好的图形性能。

Apple 于 2014 年推出了⾃家叫做 Metal 的低开销 API，Metal ⾸先⽤于移动设备，除了提⾼效率之外，减少 CPU 的占⽤还可以降低功耗，Metal 有着属于⾃⼰的着⾊器编程语⾔，同时适⽤于图形程序和 GPU 计算程序。

AMD 将⾃身 Mantle 的⼯作贡献给了 Khronos 组织，后者于 2016 年推出了新⼀代的 API，叫做 Vulkan。与 OpenGL ⼀样，Vulkan 可以⽤于多个操作系统。Vulkan 使⽤了⼀种被称为 SPIR-V 的全新⾼级中间语⾔，它可以同时⽤于着⾊器表示和通⽤GPU 计算。这些预编译的着⾊器代码是可移植的，因此可以在任何⽀持该功能特性的 GPU 上进⾏使⽤。Vulkan 也可以被⽤于⾮图形的 GPU 计算，这些计算通常并没有⼀个⽤于显示画⾯的窗⼝。与其他低开销驱动的 API 相⽐，Vulkan 的⼀个显著区别在于，它具有⼗分强⼤的跨平台特性，可以在从⼯作站到移动设备的很多系统上进⾏使⽤。

在移动设备上⼀般会使⽤ OpenGL ES，其中“ES”代表的是嵌⼊式系统（embedded system），因为这个 API 是针对移动设备进⾏开发的；这是由于标准 OpenGL 中的⼀些调⽤结构⼗分臃肿和缓慢，并且还需要对其中很少使⽤到的功能进⾏⽀持。OpenGL ES 的⼀个分⽀是基于浏览器的 WebGL，它通过 JavaScripts 进⾏调⽤。

#### 顶点着色器
顶点着⾊器是功能流⽔线中的第⼀个阶段。虽然这是程序员可以直接进⾏控制的第⼀个阶段，但是值得注意的是，在进⼊这个阶段之前，就已经存在⼀些数据计算了。这在 DirectX 中叫做输⼊汇编器（input assembler），⼏个数据流被编织在⼀起，形成了顶点集合和图元集合，并向下发送给管线。

⼀个三⻆形⽹格由⼀组顶点构成，每个顶点都对应物体表⾯上的⼀个特定位置。除了位置之外，每个顶点上还具有⼀些其他可选的属性，例如颜⾊和纹理坐标。表⾯法线同样也会在⽹格顶点的位置上进⾏定义，虽然这看起来有点奇怪。在数学上，每个三⻆⾯都有⼀个明确定义的表⾯法线，⽽且可以直接使⽤这个三⻆形法线来进⾏着⾊计算，这看起来是更加合理的。但是在渲染过程中，三⻆形⽹格通常会被⽤来表示⼀个潜在的曲⾯，⽽顶点法线则被⽤来表示这个曲⾯的朝向，⽽不是这个三⻆形⽹格本身的朝向。所以一个顶点甚至可能有两个法线用来表示曲面上的折痕。

顶点着⾊器是处理这些三⻆形的第⼀个阶段。但是⽤于描述三⻆形是如何组成的数据（点和点之间的关系）对于顶点着⾊器⽽⾔是不可⽤的，正如顶点着⾊器的字⾯意思，它只会对传⼊的顶点进⾏处理。
顶点着⾊器提供了⼀种⽤于修改、创建或者忽略三⻆形顶点数据的⽅法，这些数据可以是颜⾊、法线、纹理坐标和位置等。通常顶点
着⾊器程序会将顶点从模型空间变换到⻬次裁剪空间中，在最极端的情况下，顶点着⾊器也必须要输出顶点的位置。

顶点着⾊器与之前所描述的统⼀着⾊器模型⾮常相似，每个传⼊的顶点都会经过顶点着⾊器的处理，然后该程序会输出⼀些数据，这些数据将会⽤于三⻆形或者线段的插值。顶点着⾊器⽆法创建或者销毁顶点，并且⼀个顶点上的计算结果也⽆法传递给另⼀个顶点，因为每个顶点都是单独处理的，GPU 上任意数量的着⾊处理器都可以并⾏的应⽤于输⼊的顶点流上。

顶点着色器应用：
物体⽣成：仅创建⼀次模型，并通过顶点着⾊器对其进⾏变形。
使⽤蒙⽪技术和变形技术来设置⻆⾊的身体动画和⾯部动画。
程序化变形：例如旗帜、布料和⽔⾯的运动。
粒⼦创建：通过向流⽔线发送简并（⽆⾯积）⽹格，并根据需要来设定它们的位置，从⽽来模拟粒⼦效果。
透镜畸变、热雾、⽔波纹、书⻚卷曲以及其他特效，可以通过将整个帧缓冲的内容作为⼀个纹理，然后将其应⽤在⼀个正在经历变形，并且屏幕对⻬的⽹格上进⾏实现。
通过使⽤顶点纹理来获取并应⽤地形的⾼度场。

#### 曲⾯细分阶段
曲⾯细分阶段允许我们绘制曲⾯，GPU 的任务就是将每个曲⾯描述都转换成⼀组三⻆形。


使⽤曲⾯细分阶段有⼏个好处。
- 描述⼀个曲⾯往往要⽐提供三⻆形⽹格本身更加紧凑，节省内存。
- 当场景中存在⼀些不断变化的⻆⾊或者物体时，这个功能还可以防⽌ CPU 与 GPU 之间的总线带宽成为程序的性能瓶颈。
- 对于⼀个给定的相机视⻆，曲⾯细分可以⽣成适当的三⻆形数量，这样的曲⾯可以被⾼效渲染。

曲⾯细分阶段同样包含三个子阶段。
DX：壳着⾊器、曲⾯细分器和域着⾊器。
OpenGL：细分控制着⾊器、图元⽣成器（固定功能的曲⾯细分器）和细分评估着⾊器。

壳着⾊器
输入：特殊的⾯⽚图元。它包含了若⼲个定义细分表⾯、Bezier ⾯⽚、以及其他类型曲线元素的控制点。
功能：
- 它会告诉细分器需要⽣成多少个三⻆形，以及如何对它们进⾏配置
- 它会对每个控制点进⾏处理。壳着⾊器也可以选择对输⼊的⾯⽚进⾏修改，根据要求添加或者移除⼀些控制点。
输出：
- 壳着⾊器发送给曲⾯细分器的⼀个重要参数是曲⾯细分因⼦（在 OpenGL 中叫做曲⾯细分等级），它有两种类型，分别是内边缘和外边缘。内边缘因⼦有两个，它决定了在三⻆形或者四边形内部进⾏细分的次数；外边缘因⼦决定了每个外部边缘被分割的次数。通过对参数的分开控制，⽆论曲⾯内部是如何细分的，我们都可以让相邻的曲⾯边界与曲⾯细分相匹配，这种边缘匹配避免了在⾯⽚相接触的地⽅产⽣裂缝，或者是产⽣其他的着⾊瑕疵。这些顶点也会被指定重⼼坐标，它代表了顶点在曲⾯上的相对位置。
- 壳着⾊器会给曲⾯细分器发送⼀些额外信息，来告诉它我们需要的是哪⼀种细分曲⾯：三⻆形、四边形还是等值线（其中等值线是⼀组线条，有时候也会⽤于头发渲染）。
- 壳着⾊器会将处理好的控制点和曲⾯细分相关的控制数据⼀起发送给域着⾊器。

曲面细分器
输入：曲⾯细分因⼦，曲⾯细分类型。
功能：⽣成顶点并计算它们的位置，指定它们所构成的三⻆形和线段。为了提⾼计算效率，这个数据放⼤的过程会在着⾊器之外执⾏。
输出：曲⾯细分点集

域着⾊器
输入：壳着⾊器的曲⾯控制点、和曲面细分器的曲⾯细分点集
功能：每个顶点⽣成重⼼坐标，并在⾯⽚的计算⽅程中使⽤重⼼坐标来⽣成顶点的位置、法线、纹理坐标以及其他需要的顶点信息。
输出：域着⾊器具有⼀个和顶点着⾊器类似的数据流模式，来⾃曲⾯细分器的每个顶点都会被处理，并⽣成⼀个相应的输出顶点。然后⽣成的三⻆形会被输⼊到管线中的下⼀个阶段。


#### ⼏何着⾊器
⼏何着⾊器可以将⼀种图元转换为另⼀种图元。
例如：我们可以为每个三⻆形都创建边界线段，从⽽将⼀个三⻆形⽹格转换成⼀个线框模型。或者我们可以使⽤⾯向观察者的狭⻓四边形来替换边界线段，从⽽⽣成⼀个具有较粗边界的线框渲染。

输⼊：⼀个独⽴物体和与其相关联的顶点。这些物体通常由⼀个条状三⻆形、⼀个线段或者仅仅是⼀个点组成，其他扩展的图元也可以在⼏何着⾊器中进⾏定义和处理。特别地，可以传⼊⼀个三⻆形之外的三个附加顶点；并且可以使⽤⼀条折线上的两个相邻顶点。

⼏何着⾊器会对这些输⼊的图元进⾏处理，会输出 0 个或者更多数量的顶点，这些顶点可以是点、折线或者条状三⻆形。请注意，在⼏何着⾊器中⽣成的图元不可以直接输出，但是可以通过编辑顶点、添加新图元和移除其他图元的⽅式，来对⽹格进⾏选择性的修改。

⽬的：对输⼊的顶点数据进⾏修改，或者是创建有限数量的副本。
⽤途例子：
⽣成六个变换后的数据副本，从⽽同时渲染⼀个⽴⽅体的六个⾯。
⽤于⾼效的创建级联阴影贴图，从⽽⽣成⾼质量的阴影。
其他利⽤⼏何着⾊器的算法包括：从点数据中创建⼤⼩可变的粒⼦；沿着模型轮廓挤压出鳍⽚从⽽模拟⽑发渲染；找到物体的边缘从⽽⽤于阴影算法等。

使⽤实例化的功能：它允许⼏何着⾊器在任意给定的图元上执⾏⼀定次数。⼏何着⾊器最多可以输出四个数据流，每个数据流都可以被发送到渲染管线的下⼀阶段进⾏处理，所有这些数据流都可以选择性地发送到流式输出的渲染⽬标中。

⼏何着⾊器会保证按照图元的输⼊顺序来输出图元。这个排序会对执⾏性能产⽣影响，因为如果有很多个着⾊器核⼼并⾏执⾏的话，那么为了保证图元的输出顺序与输⼊顺序相同，则必须要将所有执⾏后的结果保存下来并进⾏排序。这个因素和其他的⼀些因素⼀起，不利于⼏何着⾊器在⼀次调⽤中⼤量复制或者创建图形。

当⼀次 draw call 命令被提交之后，渲染管线中只有三个地⽅可以在 GPU 上创建⼯作：光栅化、曲⾯细分着⾊器和⼏何着⾊器。其中考虑到所需要的资源和内存，⼏何着⾊器的⾏为是最不可预测的，因为它是完全可编程的。在实践中，⼏何着⾊器很少会被使⽤，因为它和 GPU 并⾏计算的优势并不相符；在⼀些移动设备上，⼏何着⾊器是使⽤软件进⾏实现的，因此在这些设备上也不⿎励使⽤⼏何着⾊器。

##### 流式输出
使⽤ GPU 管线的标准⽅式是通过顶点着⾊器向 GPU 发送数据，然后将⽣成的结果三⻆形进⾏光栅化，最后在像素着⾊器中对这些数据进⾏处理。在过去，数据总是会直接穿过整个管线，直接输出到屏幕上，中间⽣成的数据都⽆法进⾏访问。

流式输出的想法是在 Shader Model 4.0 中引⼊的，在顶点被顶点着⾊器处理完成之后（这⾥还可以选择曲⾯细分和⼏何着⾊器），这些数据除了被发送到光栅化阶段之外，还可以通过⼀个流（即⼀个有序数组）来进⾏输出。事实上，我们还可以完全关闭光栅化阶段，然后将管线作为⼀个纯粹的、⾮图形的流处理器。这些处理过的数据可以通过流式输出从管线中返回，从⽽允许对其进⾏迭代处理。这类操作在模拟流动的⽔体，或者其他粒⼦特效的时候⼗分有⽤。它还可以⽤于对模型进⾏蒙⽪操作，然后让这些顶点数据可以重复使
⽤。

流式输出只能以浮点数的形式返回数据，因此它可能会占⽤很多存储空间。流式输出是作⽤于图元的，⽽不是直接作⽤于顶点的，这就意味着如果我们将⼀个⽹格输⼊到管线中，这个⽹格中的每个三⻆形都会⽣成⾃⼰的集合，每个集合都会包含三个输出的顶点，⽽原始⽹格中任何共享的顶点都会丢失。
因此，在实际使⽤中，通常会直接将顶点作为⼀个点集图元直接输⼊到管线中。在 OpenGL 中流式输出被叫做变换反馈，因为流式输出的⼤部分⽤途都是对顶点进⾏变换，然后再将它们返回进⾏其他处理。在流式输出中，图元保证会按照它们的输⼊顺序进⾏输出，这意味着流式输出需要对输⼊到管线中的顶点顺序进⾏维护

#### 像素着⾊器
光栅化是管线中相对固定的处理步骤，它不具备任何可编程性，仅仅是某些地⽅可以进⾏⾃定义配置。在这⼀步中，会对每个三⻆形进⾏遍历，从⽽确定它所覆盖的像素，光栅化器也会粗略计算⼀个三⻆形所覆盖的像素单元格⾯积。三⻆形中部分与像素重叠、或者完全与像素重叠的部分被叫做⼀个⽚元。

跨三⻆形执⾏的插值操作是由像素着⾊器程序所指定的。通常我们会使⽤透视正确的插值，这样像素表⾯位置之间的世界空间距离，就会随着物体之间距离的增加⽽增加，即物体的深度值呈线性分布。（废话）我们还可以使⽤其他类型的插值操作，例如屏幕空间插值，它不会考虑透视投影所带来的影响。

在编程中，顶点着⾊器程序的输出，在经过三⻆形（或者线段）插值之后，会成为像素着⾊器程序的输⼊。随着 GPU 的不断发展，其他的⼀些输⼊数据也逐渐暴露给了像素着⾊器，例如：可以在像素着⾊器中使⽤⽚元的屏幕空间位置；以及会有⼀个 flag 来标识三⻆形的哪⼀侧是可⻅的，这⼀点有时候是很重要的。例如在⼀个 pass 中，我们想要在⼀个三⻆形的正⾯和背⾯渲染不同材质的时候，这个 flag 会很有⽤。

有了这些输⼊数据，通常像素着⾊器就可以计算并输出⼀个⽚元的颜⾊值。它也可以⽣成⼀个不透明度或者选择性修改⽚元的深度值，在后续的合并阶段中，这些数值可以⽤于修改存储在像素上的值。光栅化阶段⽣成的深度值也可以在像素着⾊器中进⾏修改。模板缓冲通常是不可修改的，⽽是会直接将其发送给合并阶段；在 DirectX 11.3 中，也允许着⾊器对模板缓冲进⾏修改。

像素着⾊器还有⼀个独有能⼒，那就是将⼀个输⼊⽚元丢弃，也就是说不⽣成任何输出。在过去的固定渲染管线中，裁剪平⾯是⼀个只能进⾏配置的选项，后来则可以在顶点着⾊器中进⾏指定。随着我们拥有了可以在像素着⾊器中丢弃⽚元的能⼒，这个功能还可以以任何我们想要的⽅式进⾏实现，例如决定裁剪空间是“与”操作还是“或”操作。

在早期 GPU 管线中，像素着⾊器只能将结果输出到合并阶段，然后直接显示在屏幕上。随着时间的推移，像素着⾊器可以执⾏的指令数量⼤⼤增加，这种增⻓催⽣了多重渲染⽬标（multiple render target，MRT）的想法。像素着⾊器并不会直接将⽣成的结果输出到颜⾊缓冲和 z-buffer 中，⽽是会为每个⽚元⽣成多组数值，并存储到不同的缓冲区中，每个缓冲区被称为⼀个渲染⽬标（render target，RT）。

渲染⽬标通常具有相同的 x, y 维度，有些 API 也允许其拥有不同的尺⼨，但是渲染区域只会以其中最⼩的那个维度为准。某些架构会要求所有的渲染⽬标都具有相同的位深度，甚⾄要求必须是完全⼀样的数据格式。根据 GPU 的不同，能够使⽤的渲染⽬标的数量⼀般是 4 个或者 8 个。

即使有着种种限制，但是 MRT 仍然是⼀种强⼤的⼯具，可以⽤于⾼效执⾏各种算法。我们可以在⼀个 pass 中进⾏如下操作：在第⼀个渲染⽬标中⽣成颜⾊图像，在第⼆个渲染⽬标中⽣成对象标识符，并在第三个渲染⽬标中⽣成世界空间距离。MRT 的这种能⼒催⽣了⼀种不同类型的渲染管线，它被称作延迟着⾊，在延迟着⾊中，可⻅性计算和着⾊计算是在两个单独的 pass 中完成的。第⼀个 pass 计算并存储了每个像素上的物体位置和材质信息，并在之后的pass 中来⾼效计算光照以及其他效果。

像素着⾊器也有⼀些限制，通常它只能在输⼊⽚元的对应位置上来写⼊渲染⽬标，它⽆法读取相邻像素上的计算结果。也就是说，当执⾏⼀个像素着⾊器程序的时候，它既不能将⾃⼰的结果输出到相邻像素上，也⽆法访问相邻像素当前的变化；相反，它的计算结果只会对其⾃身的像素产⽣影响。但是，像素着⾊器的这种限制并不像它听起来这么严重。我们在⼀个 pass 中输出图像之后，可以在之后 pass 的像素着⾊器中，来访问图像中的任何数据；相邻像素也可以使⽤⼀些图像处理技术来进⾏处理。

刚才我们提到，像素着⾊器⽆法得知或者影响相邻像素的计算结果，但是这个规则也有⼀些例外的情况。其中⼀个情况是，像素着⾊器可以在计算梯度或者导数的时候，获取相邻⽚元的信息。像素着⾊器提供了任何插值数据在 x, y ⽅向上每个像素的变化量，这些数据对于各种计算和纹理寻址⽽⾔都⼗分有⽤，其中的梯度信息对于纹理过滤来说尤其重要。在纹理过滤中，我们需要知道⼀个像素到底覆盖了纹理图像上的多⼤⾯积。所有的现代 GPU 都通过处理 2 × 2 的⽚元组（被称作⼀个 quad，四边形）来实现这个特性。当像素着⾊器需要获取⼀个梯度数据的时候，便会返回相邻⽚元之间的差异信息。统⼀的处理器核⼼具备访问相邻数据的能⼒，这些数据被保存在同⼀个 warp 上的不同线程中，因此可以计算出梯度信息并在像素着⾊器中进⾏使⽤。这种实现的⼀个后果是，在受到动态流程控制影响的着⾊器部分，将⽆法访问梯度信息，即“if”语句或者是包含⼀定迭代次数的循环语句。同⼀组中的所有⽚元，都必须使⽤同⼀组指令来进⾏处理，以便所有四个像素的结果对于计算梯度都具有意义。这是⼀个基础性的限制，它甚⾄存在于离线渲染系统中。

DirectX 11 引⼊了⼀种缓冲类型，它叫做⽆序访问视图（UAV），这个缓冲允许在其任何位置上进⾏写⼊。这个存储缓冲区会被所有的像素着⾊器共享。
共享就会有数据冲突问题。虽然GPU可以通过着⾊器专⽤的原⼦操作，来避免这个问题，但是原⼦操作意味着，当某个内存位置正在被另⼀个着⾊器读写时，另⼀个等待访问的着⾊器此时将会停滞下来。
虽然原⼦操作避免了数据冲突问题，但是很多算法实际上都需要⼀个特定的执⾏顺序。在⼀个标准的渲染管线中，⽚元处理的结果会在合并阶段之前进⾏排序。在 DirectX 11.3 中引⼊了光栅器有序视图（ROV），它强制保证了执⾏的顺序。
ROV 和 UAV 相类似，它们都以相同的⽅式进⾏读写，它们之间最主要的区别在于，ROV 保证会以特定的顺序来访问数据，这⼤⼤增加了这些缓冲区（着⾊器可以进⾏访问）的实⽤性。
例如：ROV 可以使得像素着⾊器来编写⾃⼰的混合⽅法，⽽不需要通过合并阶段来完成，因为它可以直接访问并写⼊ ROV 中的任何位置。它的代价就是，如果检测到了⼀个⽆序访问，那么像素着⾊器的调⽤就会停滞下来，直到在它之前绘制的三⻆形被处理完成为⽌。

#### 合并阶段
在合并阶段中，我们会将每个独⽴⽚元的颜⾊和深度进⾏组合，并最终形成帧缓冲。
DirectX 将这个阶段叫做输出合并；OpenGL 将其称为逐样本操作。
在⼤多数传统渲染管线的示意图中（包括本书中的），模板缓冲和深度缓冲的相关操作都将在这个阶段执⾏。
如果⼀个⽚元是可⻅的，那么这个阶段将会发⽣的另⼀个操作是颜⾊混合。当然，对于⼀个不透明表⾯⽽⾔，并没有发⽣真正意义上的颜⾊混合，仅仅是将之前存储的颜⾊替换成了当前⽚元的颜⾊⽽已，真正的⽚元混合和颜⾊存储，通常会发⽣在透明度和合成操作。

为了避免无意义的性能浪费（比如像素着⾊器计算的片元被其他片元遮挡，在zbuffer测试时丢弃），许多 GPU 都会在执⾏像素着⾊器之前，进⾏⼀些合并测试。⽚元的深度值可以⽤于对可⻅性进⾏测试，不可⻅的⽚元将会被直接剔除，这个功能被称作为 early-z

#### 计算着⾊器
GPU 不仅可以⽤来实现传统的图形渲染管线，还可以⽤于很多⾮图形的领域，例如⽤于计算股票期权的估计价值，以及⽤于训练深度学习的神经⽹络等，这种使⽤硬件的⽅式叫做 GPU 计算（GPU computing）。

DirectX 11 引⼊了计算着⾊器（compute shader），它是利⽤ GPU 进⾏计算的⼀种⽅式。计算着⾊器是⼀种特殊的着⾊器，但是它并没有锁定在图形管线中的固定位置。它与渲染的过程密切相关，因为它是通过图形 API 来进⾏调⽤的。计算着⾊器与顶点着⾊器、像素着⾊器以及其他着⾊器可以⼀起进⾏使⽤，它利⽤了管线中相同的统⼀着⾊器处理器池。与其他着⾊器⼀样，它也有⼀组输⼊数据，并且也可以访问输⼊缓冲区和输出缓冲区（例如纹理）。warp 和线程的概念在计算着⾊器中会更加明显，例如：每个计算着⾊器的调⽤，都会获得⼀个可以访问的线程 ID。

计算着⾊器的其中⼀个优势在于，它可以访问在 GPU 上⽣成的数据。由于在 GPU和 CPU 之间进⾏通讯是⼀件效率很低的事情，因此如果我们能够将数据驻留在 GPU上，并在 GPU 上进⾏计算，那么就可以⼤幅提⾼性能表现。
计算着⾊器的⼀个普遍⽤途就是后处理计算，即以某种⽅式来对图像进⾏修改。线程之间共享内存，这意味着某些图像采样的中间结果可以与相邻线程进⾏共享。



