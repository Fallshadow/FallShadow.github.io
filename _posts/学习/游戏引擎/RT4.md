
所谓实时渲染，指在计算机上快速⽣成图像。就是将三维世界实时渲染为一张张二维图像。更底层地说法：将一堆三维物体放到虚拟空间中，引入光源作用，再用摄像机从一位置以某一规则观察这些物体，将收录到的信息转换为屏幕上一个个带颜色的像素点，以此生成一张二维图像。按照现代的做法，这一整个过程，被称为图形渲染管线。  
渲染管线的核⼼功能就是利⽤给定的虚拟相机、三维物体、光源等信息，来⽣成或者渲染⼀张⼆维图像。最终图像中物体的位置和形状，由其⼏何结构，环境特征以及相机位置所决定。⽽物体的外观则会受到材质属性、光源、纹理（应⽤在物体表⾯上的图像）以及着⾊⽅程的影响。  

图形渲染管线是一个流⽔线的过程，我们知道在理想状态下，⼀个⾮流⽔线化的系统可以被划分为 n 个流⽔线阶段，从⽽提升 n 倍的速度。拿我们人眼观察世界来举例的话，光传播到我们的眼睛是持续的过程，我们大脑从眼睛中读取图像成品是一个需求，这个需求下达到眼睛，眼睛是立刻把生成好的图片发送给大脑的，几乎可以假定其原料以及加工是瞬时的，无消耗的，自然可以说用不上流水线。而计算机观察虚拟世界，需要知道此时三维物体的位置，然后还要进行捕获、加工等等，最后生成出一个图像肯定是很慢的，在这之前屏幕都是黑的，然后有了第一副画面，可以说这第一幅画面是迟到的画面，因为它展示的是加工耗时之前的画面，但是计算机发展到现在，这样的时间已经远远大于人类的反应时间，所以我们还以为这是“现在”的画面，那么第二幅画面呢？我们是否要重新开始获取信息开始一整个流程？当然可以，但是大可不必，我们完全可以把一整个流程细分，在其中第一阶段加工完第一幅画面所需要的内容之后就可以立刻获取新的信息来为第二幅画面做准备。

渲染管线分为四个阶段：应用阶段、几何处理阶段、光栅化阶段、像素处理阶段。

## 应用阶段
由应用程序驱动、在软件中实现、运行在CPU上。负责责碰撞检测，全局加速算法，动画，物理模拟，处理输入等任务，具体会执⾏哪些任务取决于应⽤程序的类型。  
> 有⼀些应⽤阶段中的任务也可以让 GPU 来进⾏执⾏，即通过使⽤⼀个叫做计算着⾊器（compute shader）的独⽴模式，该模式会将 GPU 视为⼀个⾼度并⾏的通⽤处理器，⽽忽略其专⻔⽤于图形渲染的特殊功能。

说明一下加速算法，比如特殊的剔除算法以及渲染管线剩余部分⽆法处理的⼀切问题。
在应⽤阶段的最后，需要进⾏渲染的⼏何物体会被输⼊到⼏何处理阶段中，这些⼏何物体被称作为渲染图元，即点、线和三⻆形。这些图元最终可能会出现在屏幕上，这也是应⽤阶段中最重要的任务。

## ⼏何处理阶段
运行在GPU上。负责处理变化、投影等几何处理问题。这个阶段需要计算哪些物体会被绘制，应该如何进⾏绘制，以及应当在哪⾥绘制等问题。
运⾏在 GPU 上的⼏何处理阶段会负责⼤部分的逐三⻆形和逐顶点操作。将⼏何处理阶段再细分下去，可以划分为以下⼏个功能性阶段：顶点着⾊、投影、裁剪和屏幕映射。
#### 顶点着色
在早些时候，物体的光照是逐顶点计算的，通过将光源应⽤于每个顶点的位置和法线，从⽽计算并存储最终的顶点颜⾊；然后再通过对顶点颜⾊进⾏插值，来获取三⻆形内部像素的颜⾊，因此这个可编程的顶点处理单元被命名为顶点着⾊器。
随着现代 GPU 的出现，以及⼏乎全部的着⾊计算都在逐像素的阶段进⾏，因此顶点着⾊阶段变得越来越通⽤，甚⾄可能并不会在该阶段中进⾏任何的着⾊计算。顶点着⾊器如今是⼀个更加通⽤的单元，它负
责计算并设置与每个顶点都相关的数据。

顶点着⾊的任务主要有两个，⼀个是计算顶点的位置，另⼀个是计算顶点的附加数据，例如法线和纹理坐标等。  
##### 计算顶点的位置
⾸先我们描述⼀下顶点位置是如何被计算出来的，它需要⼀组顶点坐标来作为输⼊。

这些顶点一开始位于模型自身的模型空间中；
使用模型变换对模型的顶点和法线进行变换，将此模型转换到世界坐标系下；
然而，只有被相机看到的模型才会被渲染到屏幕上，所以再对模型进行视图变换，使模型位于相机空间下；
##### 计算顶点的附加数据
为了创建⼀个真实的场景，仅仅是渲染物体的位置和形状是不够的，我们还需要对物体的外观信息进⾏建模，包括物体的材质（material）信息以及光源照射在物体表⾯上的效果。从最简单的颜⾊描述到基于
物理的详细描述，材质和光源可以通过很多⽅式进⾏建模。确定光照作⽤于材质上所产⽣的效果，这个操作被称为着⾊，它涉及到在模型的不同位置上计算着⾊⽅程。
#### 投影
投影会将可视空间变换为一个标准立方体，可以使用不同的范围来定义相同的空间，例如0≤z≤1，这个标准立方体被称为规范可视空间。

投影操作在GPU上的顶点着色器中完成，有两种常见的投影方法：正交投影，透视投影。
正交投影是平⾏投影中的⼀种，平⾏投影常用的还有斜平⾯投影和轴测投影，正交视图的可视空间通常是⼀个⻓⽅体，正交投影会将这个可视空间变换为⼀个标准⽴⽅体。正交投影最主要的特征就是，投影变换之前的平⾏线，在正交投影之后仍然是平⾏的。这样的投影变换由⼀个位移变换和⼀个缩放变换组成。
在透视投影中，距离相机越远的物体，在投影变换之后就越⼩；此外，平⾏线在透视投影之后也会在视界处汇聚，也就是说，透视投影模拟了我们感知物体⼤⼩的⽅式。透视投影的可视空间（也叫做视锥体）是⼀个具有矩形底⾯的截断⾦字塔，这个视锥体也会被投影变换为⼀个标准⽴⽅体。

正交投影和透视投影都可以使⽤⼀个4×4矩阵来进⾏描述，在投影变换之后，模型所处的坐标系被称为裁剪坐标系，在坐标除以w分量之前，事实上它们都是⻬次坐标，我们将会在第4章中进⾏详细讨论。

尽管这些投影变换矩阵会将模型从⼀个空间变换到另⼀个空间，但是它们仍然被叫做投影，这是因为在显示之后，坐标的 z 分量并不会被存储在⽣成的图像中，⽽是存储在⼀个叫做z-buffer的地⽅，详⻅章节 2.5。通过这种⽅式，模型便从三维空间投影到了⼆维空间中。

#### 可选的顶点处理
每个渲染管线中，都会有刚才所描述的顶点处理阶段，当完成顶点处理之后，还有⼏个可以在 GPU 上执⾏的可选操作，它们的执⾏顺序如下：曲⾯细分、⼏何着⾊和流式输出。是否使⽤这些可选操作，⼀⽅⾯取决于硬件的功能（并不是所有 GPU 都⽀持这些功能），另⼀⽅⾯取决于程序员的意愿。这些功能相互独⽴，⽽且⼀般并不是很常⽤。

第⼀个可选阶段是曲⾯细分：想象现在有⼀个使⽤三⻆形面片组合来进⾏表示的弹性⼩球，我们可能会遇到质量和性能的取舍问题。在远处观察这个⼩球可能看起来会很不错，但是如果离近了看，我们会发现部分三⻆形，尤其是⼩球轮廓边缘处的三⻆形会⾮常明显。如果我们给这个⼩球添加更多的三⻆形来提⾼表现质量，那么当这个⼩球距离相机很远，仅仅占据屏幕上⼏个像素的时候，我们会浪费⼤量的计算时间和内存。这个时候，使⽤曲⾯细分可以为⼀个曲⾯⽣成数量合适的三⻆形，同时兼顾质量和效率。
曲⾯细分阶段本身也包含了⼀系列⼦阶段：壳着⾊器、曲⾯细分器和域着⾊器，它们可以将当前的顶点集合转换为更⼤的顶点集合，从⽽创建出更多的三⻆形。场景中的相机位置可以⽤来决定需要⽣成多少个三⻆形：当距离相机很近时，则⽣成较多数量的三⻆形；当距离相机很远时，则⽣成较少数量的三⻆形。

下⼀个可选阶段是⼏何着⾊器：和曲⾯细分相似的是它也将各种类型的图元作为输⼊，然后⽣成新的顶点。但它是⼀个较为简单的阶段，因为它能够创建的范围是有限的，能够输出的图元则更加有限。⼏何着⾊器有好⼏种⽤途，其中最流⾏的⼀种就是⽤来⽣成粒⼦。
想象我们正在模拟⼀个烟花爆炸的过程，每颗⽕花都可以表示为⼀个点，即⼀个简单的顶点。⼏何着⾊器可以将每个顶点都转换成⼀个正⽅形（由两个三⻆形组成），这个正⽅形会始终⾯朝观察者，并且会占据若⼲
个像素，这为我们提供了⼀个更加令⼈信服的图元来进⾏后续的着⾊。

最后⼀个可选阶段叫做流式输出：这个阶段可以让我们把 GPU 作为⼀个⼏何引擎，我们可以选择将这些处理好的数据输⼊到⼀个缓冲区中，⽽不是将其直接输⼊到渲染管线的后续部分并直接输出到屏幕上，这些缓冲区中的数据可以被CPU读回使⽤，也可以被GPU本身的后续步骤使⽤。这个阶段通常会⽤于粒⼦模拟，例如我们刚才所举的烟花案例。

最终我们都会拥有⼀组使⽤⻬次坐标进⾏表示的顶点，并输⼊到管线的下⼀阶段中，并检查这些顶点是否会被相机看⻅。

#### 裁剪
完全位于可视空间内部的图元，将会按照原样传递给下⼀阶段；完全位于可视空间之外的图元，将不会传递给下⼀阶段，因为它们是不可⻅的，也不会被渲染；⽽对于那些⼀部分位于可视空间内部，⼀部分位于可视空间外部的图元，则需要进⾏额外的裁剪操作。例如：⼀个顶点在外，⼀个顶点在内的线段会被可视空间裁剪，裁剪之后会⽣成⼀个新的顶点，⽤来替代可视空间之外的那个顶点，这个新顶点位于线段和可视空间的
交点处。

除了可视空间的六个裁剪平⾯之外，⽤户还可以定义额外的裁剪平⾯来对物体进⾏裁剪，这种操作被称为切⽚。

这⾥我们会使⽤投影变换⽣成的四维⻬次坐标，来完成这个剪切操作，⻬次坐标在透视空间中的三⻆形上进⾏的插值，通常并不是线性的，同时我们需要使⽤⻬次坐标的第四个值，以便在透视投影之后进⾏正确的插值和裁剪。最后会进⾏透视除法，将得到的三⻆形位置转换到三维标准化设备坐标系中。

前⽂中我们提到，这个标准⽴⽅体的范围是（-1，-1，-1）到（1，1，1） ，⼏何处理阶段的最后⼀步就是将这个空间转换为窗⼝坐标系。

#### 屏幕映射
只有位于可视空间内部的图元才会被传递到屏幕映射阶段。当这些图元进⼊这⼀阶段时，其坐标还是三维的，其中x坐标和y坐标会被转换为屏幕坐标，屏幕坐标和z坐标在⼀起，被称作窗⼝坐标。

屏幕映射包含了⼀个缩放操作，映射后的新x，y坐标会被称为屏幕坐标。z坐标同样也会被映射到[z1,z2]的范围中，默认是z1=0,z2=1（OpenGL 中的范围是-1,1 ，DirectX 中的范围是0,1），但是这些范围也可以使⽤相应的API来进⾏修改。窗⼝坐标和这个被重映射的z值⼀起，都会被传递到光栅化阶段中。

## 光栅化阶段
现在我们已经有了被正确变换和正确投影的顶点数据，以及它们相应的着⾊数据（在⼏何处理阶段中获取的），下⼀阶段的⽬标是找到位于待渲染图元中的所有像素值。我们将这个过程称作为光栅化，我们可以将其划分成两个⼦阶段：三⻆形设置（图元装配）和三⻆形遍历。
这⾥需要注意的是，这两个⼦阶段也同样适⽤于点和线，只是因为三⻆形是更加常⻅的图元，因此这两个⼦阶段的名字中才带有了三⻆形。

光栅化也被称为扫描变换，这是⼀个将屏幕空间中⼆维顶点，转换到屏幕上像素的过程，其中每个顶点都对应⼀个z值（深度缓冲）和各种各样的着⾊信息。

判定某个三⻆形和屏幕上的哪些像素重合，取决于我们如何实现 GPU 管线。例如：你可以使⽤点采样来判定某个点是否位于三⻆形内部。最简单的⽅式就是直接将每个像素的中⼼点来作为该像素的样本，如果该像素的中⼼点位于三⻆形内部的话，那么我们就认为该像素也位于三⻆形的内部。我们还可以通过超采样或者多重采样抗锯⻮技术，来对每个像素进⾏多次采样，详⻅章节 5.4.2。
另⼀种⽅法是使⽤保守光栅化，即当某个像素只要有⼀部分与三⻆形重叠时，我们就认为该像素位于三⻆形内部，详⻅章节 23.1.2。

#### 三角形设置（图元装配）
三⻆形的微分、边界⽅程和其他数据，都会在这个阶段进⾏计算，这些数据可以⽤于三⻆形遍历，以及对⼏何处理阶段产⽣的各种着⾊数据进⾏插值。这个功能⼀般会使⽤固定功能的硬件实现。

#### 三角形遍历
在这⼀阶段，会对每个被三⻆形覆盖的像素（中⼼点或者样本点在三⻆形内部的像素）进⾏逐个检查，并⽣成⼀个对应的⽚元（fragment）。我们可以在章节 5.4 中找到更加详细的采样⽅法。找到那些位于三⻆形内部的点或者样本，这个过程通常被称为三⻆形遍历，并且会对三⻆形三个顶点上的属性进⾏插值，来获得每个三⻆形⽚元的属性（第 5 章），这些属性包括⽚元的深度，以及⼏何阶段输出的相关着⾊数据
等。
McCormack 等⼈[1162]提供了有关三⻆形遍历的更多信息。在光栅化阶段也会对三⻆形进⾏透视正确的插值[694]（章节 23.1.1）。

⽚元内部的像素或者样本会被输⼊到像素处理阶段中，下⾯我们将会对其进⾏介绍。

## 像素处理阶段
运行在GPU上。决定每一个像素的颜色、可见与否，还可以执行逐像素的多种操作，比如将此像素新计算出来的颜色和之前的颜色进行混合。

经过之前若⼲阶段的处理，这⾥我们已经找到了所有位于三⻆形（或者其他图元）内部的像素。像素处理阶段也可以被划分为像素着⾊和合并
两个阶段。在像素处理阶段，会对图元内部的的像素（或者样本）进⾏逐像素（或者逐样本）的计算和操作。
#### 片元着色
像素着⾊阶段是由可编程的 GPU 核⼼来执⾏的。为此，程序员需要为⽚元着⾊器提供⼀个实现程序，这个程序中包含了任何我们想要的着⾊计算操作。这⾥可以使⽤各种各样的技术，其中最重要的⼀个技术就是纹理化（texturing），详⻅第 6 章。简单来说，纹理化就是将⼀个图像或者多个图像“粘合（gluing）”在物体表⾯，从⽽实现各种各样的效果和⽬的。⼀般这些纹理都是⼆维图像，但是有时候也可以是⼀维图像或者三维图像。简单来说，像素着⾊阶段最终会输出每个⽚元的颜⾊值，这些颜⾊值会被输⼊到下⼀个⼦阶段中。
#### 像素合并
颜⾊缓冲是⼀个矩形阵列，它存储了每个像素中的颜⾊信息（即颜⾊的红绿蓝分量）。在之前的片元着⾊阶段中，我们计算了每个⽚元的颜⾊，并将其存储在颜⾊缓冲中，⽽合并阶段的任务就是将这些⽚元的颜⾊组合起来。这个阶段也被叫做 ROP，意思是“光栅操作管线（raster operations pipeline）”或者“渲染输出单元（render output unit）”，这取决于你问的是谁。与像素着⾊阶段不同，执⾏这⼀阶段的 GPU ⼦单元，并不是完全可编程的；但它仍然是⾼度可配置的，可以⽀持实现各种效果。
##### 可见性问题
合并阶段还负责解决可⻅性问题，即当整个场景被渲染的时候，颜⾊缓冲应当只包含那些相机可⻅的图元颜⾊。对于⼤部分或者⼏乎所有的图形硬件⽽⾔，这个操作是通过 z-buffer（深度缓冲）实现的。z-buffer 具有与颜⾊缓冲相同的尺⼨，对于其中的每个像素，它存储了⽬前距离最近的图元 z 值。这意味着，当⼀个图元要被渲染到某个像素上时，会计算这个图元的 z 值，并将其与 z-buffer 中的对应像素深度进⾏⽐较。如果这个新的 z 值⽐当前 z-buffer 中的像素深度更⼩，说明这个新图元距离相机更近，会挡住原来的图元，因此需要使⽤新图元的 z 值和颜⾊值来对 z buffer 和颜⾊缓冲进⾏更新；

请注意，z-buffer 允许图元以任意顺序进⾏渲染，这也是它流⾏的另⼀个原因。但是 z-buffer 在每个屏幕像素上，只存储了⼀个深度值，因此它不适⽤于透明物体的渲染。透明物体必须要等到所有的不透明物体都渲染完成之后，才能进⾏渲染，⽽且需要严格按照从后往前的顺序进⾏渲
染，或者使⽤⼀个顺序⽆关的透明算法（章节 5.5）。透明物体的渲染是 z-buffer 算法的主要弱点之⼀。

##### 其他信息（例如透明通道信息）
我们刚才提到使⽤颜⾊缓冲来存储每个像素的颜⾊，使⽤ z-buffer 来存储每个像素的 z 值。但是还有⼀些其他的通道和缓冲可以⽤来过滤和捕获⽚元的信息，例如与颜⾊缓冲相关联的透明通道（alpha channel），它存储了每个像素的不透明度（opacity，章节 5.5）。在⼀些较⽼的 API 中，透明通道也可以被⽤来进⾏透明测试（alpha test），来选择性的丢弃⼀些像素。如今这样的⽚元丢弃操作可以在像素着⾊器中完成，⽽且任何我们想要的计算都可以⽤来触发这个丢弃操作。这种类型的测试可以⽤来确保那些完全透明的⽚元，不会对 z-buffer 产⽣影响（章节 6.6）。

##### 模板缓冲
模板缓冲是⼀个离屏缓冲区，它可以⽤来记录被渲染图元的位置信息，通常它的每个像素包含 8 bit。图元可以通过各种各样函数来被渲染到模板缓冲中，同时模板缓冲可以⽤来控制渲染到颜⾊缓冲和 z-buffer 中的内容。举个例⼦：假设现在有⼀个实⼼圆被写⼊到了模板缓冲中，现在我们通过⼀个操作，可以只允许后续图元被渲染到这个实⼼圆所在位置的颜⾊缓冲中。模板缓冲⼗分强⼤，可以⽤于⽣成⼀些特殊效果。

系统中的所有缓冲区在⼀起，被统称为帧缓冲（frame buffer）。
##### 混合操作
所有这些在管线末尾的功能都被叫做光栅操作（raster operation，ROP）或者混合操作（blend operation）。
我们也可以将当前颜⾊缓冲中的颜⾊，与三⻆形中正在处理的颜⾊相混合，从⽽实现⼀些透明效果或者颜⾊样本累积的效果。上⽂中我们提到，混合操作通常并不是完全可编程的，⼀般只能通过使⽤ API 来进⾏配置。但是某些 API ⽀持光栅顺序视图，也可以被称作像素着⾊器排序，它⽀持可编程的混合操作。

##### 双缓冲
当图元到达并通过光栅化阶段时，这些从相机⻆度可⻅的图元将会被显示在屏幕上，屏幕上所显示的内容就是颜⾊缓冲中的内容。由于渲染需要花费⼀定时间，为了避免观察者看到图元渲染并显示在屏幕上的过程，⼀般都会使⽤双缓冲机制，这意味着场景的渲染都会在屏幕外的后置缓冲区中进⾏。当场景被渲染到后置缓冲区之后，后置缓冲区会与显示在屏幕上的前置缓冲区交换内容。这个交换的过程通常发⽣在垂直回扫的过程中，因此这样做是可⾏的。




